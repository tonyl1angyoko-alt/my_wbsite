---
description: 'Ï€0: A Vision-Language-Action Flow Model for  General Robot Control'
---

# Î 0ï¼šè®ºæ–‡å­¦ä¹ 



1. produces continuous actions via flow matchingã€‚è¿™é‡Œçš„flow matchingæ˜¯ä»€ä¹ˆ\
   Aï¼šç›¸å½“äºdiffusionä»å™ªéŸ³åˆ°é™å™ªçš„è¿‡ç¨‹ã€‚è·å¾—ä»å™ªå£°åˆ°çœŸå®åŠ¨ä½œçš„æµåœºã€‚ä¸»åŠ¨åŠ å™ªå£°ï¼Œä½œä¸ºæ•°æ®å–‚ç»™æ¨¡å‹è®©ä»–å­¦ä¼šå»å™ªè¿‡ç¨‹
2. è®­ç»ƒå…·ä½“ä»»åŠ¡ä¹Ÿæ˜¯å…ˆè®­ç»ƒgeneralä»»åŠ¡å†è¿›è¡Œå¾®è°ƒæ•ˆæœæ›´å¥½ï¼Œä¸ºä»€ä¹ˆ
3. generalæ¨¡å‹éœ€è¦è®­ç»ƒæ•°æ®è¾¾åˆ°ä¸€å®šè§„æ¨¡å­˜åœ¨ä¸€ä¸ªå¿…é¡»çš„é˜ˆå€¼
4. training recipeçš„ä¼˜åŒ–å§‹ç»ˆæ˜¯æœ€é‡è¦çš„ä¸€æ­¥
5. VLAè®­ç»ƒæ—¶é€‰æ‹©äº†ä¸åŒå…·èº«çš„æ•°æ®çš„èåˆ
6. ä¼ ç»Ÿæ–¹æ³•ï¼šäº¤å‰ç†µç¦»æ•£çš„åŠ¨ä½œã€‚é‡‡å–flow matchingå¯ä»¥è·å¾—è¿ç»­åŠ¨ä½œã€‚\
   å€Ÿé‰´ä»¥å¾€çš„åˆ†æƒé‡ï¼ˆä¹Ÿå°±æ˜¯åˆ†æ¨¡å—ï¼‰ï¼ŒVLM+expert

<figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>

7. ä»–ä»¬å¹¶éç®€å•åœ°ç…§æ¬ Transfusion çš„æ··åˆè®­ç»ƒæ¨¡å¼ï¼Œè€Œæ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªä¸“é—¨è´Ÿè´£å¤„ç†æœºå™¨äººæœ¬ä½“çŠ¶æ€å’ŒåŠ¨ä½œçš„â€œä¸“å®¶ç½‘ç»œâ€ï¼ˆæ‹¥æœ‰ç‹¬ç«‹çš„å‚æ•°ï¼‰ï¼Œå¯¹åŸæœ‰æ€æƒ³è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶ç”¨å®éªŒè¯æ˜äº†è¿™ç§â€œä¸“ä¸šåŒ–åˆ†å·¥â€çš„è®¾è®¡èƒ½å¤Ÿå¸¦æ¥å®å®åœ¨åœ¨çš„æ€§èƒ½å¥½å¤„ã€‚
8.  æ¨¡å‹å­¦ä¹ æ ¹æ®ã€çŠ¶æ€ï¼Œæ‘„åƒå¤´ï¼Œè¯­ä¹‰ã€‘å­¦ä¹ ç»™å‡ºçš„åŠ¨ä½œAï¼Œä½†æ˜¯

    * åŠ¨ä½œå— (Action Chunk)ï¼šæ¨¡å‹ä¸€æ¬¡æ€§é¢„æµ‹å‡ºæœªæ¥ H ä¸ªæ—¶é—´æ­¥çš„å®Œæ•´åŠ¨ä½œåºåˆ— ã€‚
    * H = 50ï¼šåœ¨è¿™ç¯‡è®ºæ–‡çš„ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¼šä¸€æ¬¡æ€§ç”Ÿæˆæ¥ä¸‹æ¥è¿ç»­50æ­¥çš„åŠ¨ä½œ ã€‚è¿™ä½¿å¾—æœºå™¨äººçš„åŠ¨ä½œéå¸¸è¿è´¯å’Œæµç•…ï¼Œè€Œä¸æ˜¯ä¸€ç³»åˆ—æ–­æ–­ç»­ç»­çš„ã€å¡é¡¿çš„åŠ¨ä½œ

    â€œç¿»è¯‘â€è¿‡ç¨‹ï¼šæ¨¡å‹ä¼šç”¨ä¸åŒçš„â€œç¼–ç å™¨ (encoders)â€æŠŠè¿™ä¸‰ç§ä¸åŒæ ¼å¼çš„æ•°æ®ï¼Œâ€œç¿»è¯‘â€æˆç»Ÿä¸€çš„æ•°å­¦è¯­è¨€ï¼ˆå³â€œsame embedding spaceâ€ï¼Œç›¸åŒçš„åµŒå…¥ç©ºé—´ï¼‰



pi0torchï¼š

```python
import logging
import math

import torch
from torch import Tensor
from torch import nn
import torch.nn.functional as F  # noqa: N812

import openpi.models.gemma as _gemma
from openpi.models_pytorch.gemma_pytorch import PaliGemmaWithExpertModel
import openpi.models_pytorch.preprocessing_pytorch as _preprocessing


def get_safe_dtype(target_dtype, device_type):
    """Get a safe dtype for the given device type."""
    if device_type == "cpu":
        # CPU doesn't support bfloat16, use float32 instead
        if target_dtype == torch.bfloat16:
            return torch.float32
        if target_dtype == torch.float64:
            return torch.float64
    return target_dtype


def create_sinusoidal_pos_embedding(
    time: torch.tensor, dimension: int, min_period: float, max_period: float, device="cpu"
) -> Tensor:
    """Computes sine-cosine positional embedding vectors for scalar positions."""
    if dimension % 2 != 0:
        raise ValueError(f"dimension ({dimension}) must be divisible by 2")

    if time.ndim != 1:
        raise ValueError("The time tensor is expected to be of shape `(batch_size, )`.")

    dtype = get_safe_dtype(torch.float64, device.type)
    fraction = torch.linspace(0.0, 1.0, dimension // 2, dtype=dtype, device=device)
    period = min_period * (max_period / min_period) ** fraction

    # Compute the outer product
    scaling_factor = 1.0 / period * 2 * math.pi
    sin_input = scaling_factor[None, :] * time[:, None]
    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)


def sample_beta(alpha, beta, bsize, device):
    alpha_t = torch.as_tensor(alpha, dtype=torch.float32, device=device)
    beta_t = torch.as_tensor(beta, dtype=torch.float32, device=device)
    dist = torch.distributions.Beta(alpha_t, beta_t)
    return dist.sample((bsize,))


def make_att_2d_masks(pad_masks, att_masks):
    """Copied from big_vision.

    Tokens can attend to valid inputs tokens which have a cumulative mask_ar
    smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to
    setup several types of attention, for example:

      [[1 1 1 1 1 1]]: pure causal attention.

      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend between
          themselves and the last 3 tokens have a causal attention. The first
          entry could also be a 1 without changing behaviour.

      [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a
          block can attend all previous blocks and all tokens on the same block.

    Args:
      input_mask: bool[B, N] true if its part of the input, false if padding.
      mask_ar: int32[B, N] mask that's 1 where previous tokens cannot depend on
        it and 0 where it shares the same attention mask as the previous token.
    """
    if att_masks.ndim != 2:
        raise ValueError(att_masks.ndim)
    if pad_masks.ndim != 2:
        raise ValueError(pad_masks.ndim)

    cumsum = torch.cumsum(att_masks, dim=1)
    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]
    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]
    return att_2d_masks & pad_2d_masks


class PI0Pytorch(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.pi05 = config.pi05

        paligemma_config = _gemma.get_config(config.paligemma_variant)
        action_expert_config = _gemma.get_config(config.action_expert_variant)

        self.paligemma_with_expert = PaliGemmaWithExpertModel(
            paligemma_config,
            action_expert_config,
            use_adarms=[False, True] if self.pi05 else [False, False],
            precision=config.dtype,
        )

        self.action_in_proj = nn.Linear(32, action_expert_config.width)
        self.action_out_proj = nn.Linear(action_expert_config.width, 32)

        if self.pi05:
            self.time_mlp_in = nn.Linear(action_expert_config.width, action_expert_config.width)
            self.time_mlp_out = nn.Linear(action_expert_config.width, action_expert_config.width)
        else:
            self.state_proj = nn.Linear(32, action_expert_config.width)
            self.action_time_mlp_in = nn.Linear(2 * action_expert_config.width, action_expert_config.width)
            self.action_time_mlp_out = nn.Linear(action_expert_config.width, action_expert_config.width)

        torch.set_float32_matmul_precision("high")
        self.sample_actions = torch.compile(self.sample_actions, mode="max-autotune")

        # Initialize gradient checkpointing flag
        self.gradient_checkpointing_enabled = False

        msg = "transformers_replace is not installed correctly. Please install it with `uv pip install transformers==4.53.2` and `cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/`."
        try:
            from transformers.models.siglip import check

            if not check.check_whether_transformers_replace_is_installed_correctly():
                raise ValueError(msg)
        except ImportError:
            raise ValueError(msg) from None

    def gradient_checkpointing_enable(self):
        """Enable gradient checkpointing for memory optimization."""
        self.gradient_checkpointing_enabled = True
        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = True
        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = True
        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = True

        logging.info("Enabled gradient checkpointing for PI0Pytorch model")

    def gradient_checkpointing_disable(self):
        """Disable gradient checkpointing."""
        self.gradient_checkpointing_enabled = False
        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = False
        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = False
        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = False

        logging.info("Disabled gradient checkpointing for PI0Pytorch model")

    def is_gradient_checkpointing_enabled(self):
        """Check if gradient checkpointing is enabled."""
        return self.gradient_checkpointing_enabled

    def _apply_checkpoint(self, func, *args, **kwargs):
        """Helper method to apply gradient checkpointing if enabled."""
        if self.gradient_checkpointing_enabled and self.training:
            return torch.utils.checkpoint.checkpoint(
                func, *args, use_reentrant=False, preserve_rng_state=False, **kwargs
            )
        return func(*args, **kwargs)

    def _prepare_attention_masks_4d(self, att_2d_masks):
        """Helper method to prepare 4D attention masks for transformer."""
        att_2d_masks_4d = att_2d_masks[:, None, :, :]
        return torch.where(att_2d_masks_4d, 0.0, -2.3819763e38)

    def _preprocess_observation(self, observation, *, train=True):
        """Helper method to preprocess observation."""
        observation = _preprocessing.preprocess_observation_pytorch(observation, train=train)
        return (
            list(observation.images.values()),
            list(observation.image_masks.values()),
            observation.tokenized_prompt,
            observation.tokenized_prompt_mask,
            observation.state,
        )

    def sample_noise(self, shape, device):
        return torch.normal(
            mean=0.0,
            std=1.0,
            size=shape,
            dtype=torch.float32,
            device=device,
        )

    def sample_time(self, bsize, device):
        time_beta = sample_beta(1.5, 1.0, bsize, device)
        time = time_beta * 0.999 + 0.001
        return time.to(dtype=torch.float32, device=device)

    def embed_prefix(
        self, images, img_masks, lang_tokens, lang_masks
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Embed images with SigLIP and language tokens with embedding layer to prepare
        for PaliGemma transformer processing.
        """
        embs = []
        pad_masks = []
        att_masks = []

        # Process images
        for img, img_mask in zip(images, img_masks, strict=True):

            def image_embed_func(img):
                return self.paligemma_with_expert.embed_image(img)

            img_emb = self._apply_checkpoint(image_embed_func, img)

            bsize, num_img_embs = img_emb.shape[:2]

            embs.append(img_emb)
            pad_masks.append(img_mask[:, None].expand(bsize, num_img_embs))

            # Create attention masks so that image tokens attend to each other
            att_masks += [0] * num_img_embs

        # Process language tokens
        def lang_embed_func(lang_tokens):
            lang_emb = self.paligemma_with_expert.embed_language_tokens(lang_tokens)
            lang_emb_dim = lang_emb.shape[-1]
            return lang_emb * math.sqrt(lang_emb_dim)

        lang_emb = self._apply_checkpoint(lang_embed_func, lang_tokens)

        embs.append(lang_emb)
        pad_masks.append(lang_masks)

        # full attention between image and language inputs
        num_lang_embs = lang_emb.shape[1]
        att_masks += [0] * num_lang_embs

        embs = torch.cat(embs, dim=1)
        pad_masks = torch.cat(pad_masks, dim=1)
        att_masks = torch.tensor(att_masks, dtype=torch.bool, device=pad_masks.device)

        # Get batch size from the first dimension of the concatenated tensors
        bsize = pad_masks.shape[0]
        att_masks = att_masks[None, :].expand(bsize, len(att_masks))

        return embs, pad_masks, att_masks

    def embed_suffix(self, state, noisy_actions, timestep):
        """Embed state, noisy_actions, timestep to prepare for Expert Gemma processing."""
        embs = []
        pad_masks = []
        att_masks = []

        if not self.pi05:
            if self.state_proj.weight.dtype == torch.float32:
                state = state.to(torch.float32)

            # Embed state
            def state_proj_func(state):
                return self.state_proj(state)

            state_emb = self._apply_checkpoint(state_proj_func, state)

            embs.append(state_emb[:, None, :])
            bsize = state_emb.shape[0]
            device = state_emb.device

            state_mask = torch.ones(bsize, 1, dtype=torch.bool, device=device)
            pad_masks.append(state_mask)

            # Set attention masks so that image and language inputs do not attend to state or actions
            att_masks += [1]

        # Embed timestep using sine-cosine positional encoding with sensitivity in the range [0, 1]
        time_emb = create_sinusoidal_pos_embedding(
            timestep, self.action_in_proj.out_features, min_period=4e-3, max_period=4.0, device=timestep.device
        )
        time_emb = time_emb.type(dtype=timestep.dtype)

        # Fuse timestep + action information using an MLP
        def action_proj_func(noisy_actions):
            return self.action_in_proj(noisy_actions)

        action_emb = self._apply_checkpoint(action_proj_func, noisy_actions)

        if not self.pi05:
            time_emb = time_emb[:, None, :].expand_as(action_emb)
            action_time_emb = torch.cat([action_emb, time_emb], dim=2)

            # Apply MLP layers
            def mlp_func(action_time_emb):
                x = self.action_time_mlp_in(action_time_emb)
                x = F.silu(x)  # swish == silu
                return self.action_time_mlp_out(x)

            action_time_emb = self._apply_checkpoint(mlp_func, action_time_emb)
            adarms_cond = None
        else:
            # time MLP (for adaRMS)
            def time_mlp_func(time_emb):
                x = self.time_mlp_in(time_emb)
                x = F.silu(x)  # swish == silu
                x = self.time_mlp_out(x)
                return F.silu(x)

            time_emb = self._apply_checkpoint(time_mlp_func, time_emb)
            action_time_emb = action_emb
            adarms_cond = time_emb

        # Add to input tokens
        embs.append(action_time_emb)

        bsize, action_time_dim = action_time_emb.shape[:2]
        action_time_mask = torch.ones(bsize, action_time_dim, dtype=torch.bool, device=timestep.device)
        pad_masks.append(action_time_mask)

        # Set attention masks so that image, language and state inputs do not attend to action tokens
        att_masks += [1] + ([0] * (self.config.action_horizon - 1))

        embs = torch.cat(embs, dim=1)
        pad_masks = torch.cat(pad_masks, dim=1)
        att_masks = torch.tensor(att_masks, dtype=embs.dtype, device=embs.device)
        att_masks = att_masks[None, :].expand(bsize, len(att_masks))

        return embs, pad_masks, att_masks, adarms_cond

    def forward(self, observation, actions, noise=None, time=None) -> Tensor:
        """Do a full training forward pass and compute the loss (batch_size x num_steps x num_motors)"""
        images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=True)

        if noise is None:
            noise = self.sample_noise(actions.shape, actions.device)

        if time is None:
            time = self.sample_time(actions.shape[0], actions.device)

        time_expanded = time[:, None, None]
        x_t = time_expanded * noise + (1 - time_expanded) * actions
        u_t = noise - actions

        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(images, img_masks, lang_tokens, lang_masks)
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = self.embed_suffix(state, x_t, time)
        if (
            self.paligemma_with_expert.paligemma.language_model.layers[0].self_attn.q_proj.weight.dtype
            == torch.bfloat16
        ):
            suffix_embs = suffix_embs.to(dtype=torch.bfloat16)
            prefix_embs = prefix_embs.to(dtype=torch.bfloat16)

        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)
        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)

        att_2d_masks = make_att_2d_masks(pad_masks, att_masks)
        position_ids = torch.cumsum(pad_masks, dim=1) - 1

        # Prepare attention masks
        att_2d_masks_4d = self._prepare_attention_masks_4d(att_2d_masks)

        # Apply gradient checkpointing if enabled
        def forward_func(prefix_embs, suffix_embs, att_2d_masks_4d, position_ids, adarms_cond):
            (_, suffix_out), _ = self.paligemma_with_expert.forward(
                attention_mask=att_2d_masks_4d,
                position_ids=position_ids,
                past_key_values=None,
                inputs_embeds=[prefix_embs, suffix_embs],
                use_cache=False,
                adarms_cond=[None, adarms_cond],
            )
            return suffix_out

        suffix_out = self._apply_checkpoint(
            forward_func, prefix_embs, suffix_embs, att_2d_masks_4d, position_ids, adarms_cond
        )

        suffix_out = suffix_out[:, -self.config.action_horizon :]
        suffix_out = suffix_out.to(dtype=torch.float32)

        # Apply gradient checkpointing to final action projection if enabled
        def action_out_proj_func(suffix_out):
            return self.action_out_proj(suffix_out)

        v_t = self._apply_checkpoint(action_out_proj_func, suffix_out)

        return F.mse_loss(u_t, v_t, reduction="none")

    @torch.no_grad()
    def sample_actions(self, device, observation, noise=None, num_steps=10) -> Tensor:
        """Do a full inference forward and compute the action (batch_size x num_steps x num_motors)"""
        bsize = observation.state.shape[0]
        if noise is None:
            actions_shape = (bsize, self.config.action_horizon, self.config.action_dim)
            noise = self.sample_noise(actions_shape, device)

        images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)

        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(images, img_masks, lang_tokens, lang_masks)
        prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks, prefix_att_masks)
        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1

        # Compute image and language key value cache
        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(prefix_att_2d_masks)
        self.paligemma_with_expert.paligemma.language_model.config._attn_implementation = "eager"  # noqa: SLF001

        _, past_key_values = self.paligemma_with_expert.forward(
            attention_mask=prefix_att_2d_masks_4d,
            position_ids=prefix_position_ids,
            past_key_values=None,
            inputs_embeds=[prefix_embs, None],
            use_cache=True,
        )

        dt = -1.0 / num_steps
        dt = torch.tensor(dt, dtype=torch.float32, device=device)

        x_t = noise
        time = torch.tensor(1.0, dtype=torch.float32, device=device)
        while time >= -dt / 2:
            expanded_time = time.expand(bsize)
            v_t = self.denoise_step(
                state,
                prefix_pad_masks,
                past_key_values,
                x_t,
                expanded_time,
            )

            # Euler step - use new tensor assignment instead of in-place operation
            x_t = x_t + dt * v_t
            time += dt
        return x_t

    def denoise_step(
        self,
        state,
        prefix_pad_masks,
        past_key_values,
        x_t,
        timestep,
    ):
        """Apply one denoising step of the noise `x_t` at a given timestep."""
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = self.embed_suffix(state, x_t, timestep)

        suffix_len = suffix_pad_masks.shape[1]
        batch_size = prefix_pad_masks.shape[0]
        prefix_len = prefix_pad_masks.shape[1]

        prefix_pad_2d_masks = prefix_pad_masks[:, None, :].expand(batch_size, suffix_len, prefix_len)

        suffix_att_2d_masks = make_att_2d_masks(suffix_pad_masks, suffix_att_masks)

        full_att_2d_masks = torch.cat([prefix_pad_2d_masks, suffix_att_2d_masks], dim=2)

        prefix_offsets = torch.sum(prefix_pad_masks, dim=-1)[:, None]
        position_ids = prefix_offsets + torch.cumsum(suffix_pad_masks, dim=1) - 1

        # Prepare attention masks
        full_att_2d_masks_4d = self._prepare_attention_masks_4d(full_att_2d_masks)
        self.paligemma_with_expert.gemma_expert.model.config._attn_implementation = "eager"  # noqa: SLF001

        outputs_embeds, _ = self.paligemma_with_expert.forward(
            attention_mask=full_att_2d_masks_4d,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=[None, suffix_embs],
            use_cache=False,
            adarms_cond=[None, adarms_cond],
        )

        suffix_out = outputs_embeds[1]
        suffix_out = suffix_out[:, -self.config.action_horizon :]
        suffix_out = suffix_out.to(dtype=torch.float32)
        return self.action_out_proj(suffix_out)
```

### ç†è§£ä»£ç 

åœ¨ä»£ç ä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ç”± `embed_prefix` (åµŒå…¥å‰ç¼€) å’Œ `embed_suffix` (åµŒå…¥åç¼€) è¿™ä¸¤ä¸ªå‡½æ•°å®Œæˆã€‚æˆ‘ä»¬å…ˆçœ‹ `embed_prefix`ã€‚

#### 1. `embed_prefix`ï¼šç†è§£â€œçœ‹â€å’Œâ€œå¬â€

è¿™ä¸ªå‡½æ•°çš„å”¯ä¸€ç›®æ ‡ï¼šæŠŠå›¾åƒï¼ˆâ€œçœ‹â€ï¼‰å’Œè¯­è¨€ï¼ˆâ€œå¬â€ï¼‰è½¬æ¢æˆä¸€é•¿ä¸²çš„åµŒå…¥å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚è¿™äº›å‘é‡å°±æ˜¯æ¨¡å‹â€œå¤§è„‘â€èƒ½ç†è§£çš„è¯­è¨€ã€‚

å®ƒåŒæ—¶è¿˜ä¼šç”Ÿæˆå¯¹åº”çš„ `pad_masks` (å¡«å……æ©ç ) å’Œ `att_masks` (æ³¨æ„åŠ›æ©ç )ã€‚

```
    def embed_prefix(
        self, images, img_masks, lang_tokens, lang_masks
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Embed images with SigLIP and language tokens with embedding layer...
        """
        embs = []
        pad_masks = []
        att_masks = []
```

* è§£é‡Šï¼šåˆå§‹åŒ–ä¸‰ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨æ¥æ”¶é›†æ‰€æœ‰å‰ç¼€ï¼ˆå›¾åƒã€è¯­è¨€ï¼‰çš„ä¿¡æ¯ã€‚

**å¤„ç†å›¾åƒ**

```
        # Process images
        for img, img_mask in zip(images, img_masks, strict=True):

            def image_embed_func(img):
                return self.paligemma_with_expert.embed_image(img)

            img_emb = self._apply_checkpoint(image_embed_func, img)

            bsize, num_img_embs = img_emb.shape[:2]

            embs.append(img_emb)
            pad_masks.append(img_mask[:, None].expand(bsize, num_img_embs))
```

* `for img, ... in zip(...)`ï¼šæ¨¡å‹å¯ä»¥æ¥æ”¶å¤šä¸ªå›¾åƒï¼ˆæ¯”å¦‚3ä¸ªä¸åŒæ‘„åƒå¤´çš„è§†è§’ï¼‰ï¼Œè¿™é‡Œå®ƒä¸€ä¸ªä¸€ä¸ªåœ°å¤„ç†ã€‚
* `self.paligemma_with_expert.embed_image(img)`ï¼šè¿™å°±æ˜¯è°ƒç”¨`PaliGemma`çš„â€œè§†è§‰å¤§è„‘â€ã€‚å®ƒä¼šæŠŠä¸€å¼  `(224, 224, 3)` çš„å›¾åƒè½¬æ¢æˆä¸€å †å‘é‡ï¼Œæ¯”å¦‚ `(B, 576, D)` (Bæ˜¯æ‰¹é‡å¤§å°, 576æ˜¯å›¾åƒå—æ•°é‡, Dæ˜¯åµŒå…¥ç»´åº¦)ã€‚
* `embs.append(img_emb)`ï¼šæŠŠå›¾åƒåµŒå…¥å‘é‡ï¼ˆæ¯”å¦‚576ä¸ªï¼‰åŠ å…¥åˆ—è¡¨ã€‚
* `pad_masks.append(...)`ï¼šä¸ºè¿™576ä¸ªå‘é‡æ·»åŠ  `True`ï¼Œè¡¨ç¤ºå®ƒä»¬éƒ½æ˜¯â€œçœŸå®çš„â€tokenï¼Œä¸æ˜¯å¡«å……ã€‚

```
            # Create attention masks so that image tokens attend to each other
            att_masks += [0] * num_img_embs
```

* `att_masks += [0] * num_img_embs`ï¼šè¿™æ˜¯å…³é”®ï¼è¿˜è®°å¾— `make_att_2d_masks` é‡Œçš„ `cumsum` å—ï¼Ÿ
* é€šè¿‡åœ¨è¿™é‡Œæ·»åŠ  `num_img_embs` ä¸ª (æ¯”å¦‚576ä¸ª) `0`ï¼Œæˆ‘ä»¬æ˜¯åœ¨è¯´ï¼šâ€œæ‰€æœ‰è¿™äº›å›¾åƒtokenéƒ½å±äºåŒä¸€ä¸ªæ³¨æ„åŠ›å—â€ã€‚å®ƒä»¬çš„â€œå—IDâ€åœ¨ `cumsum` ä¹‹åéƒ½ä¼šæ˜¯ `0`ã€‚

**å¤„ç†è¯­è¨€**

```
        # Process language tokens
        def lang_embed_func(lang_tokens):
            ...
        lang_emb = self._apply_checkpoint(lang_embed_func, lang_tokens)

        embs.append(lang_emb)
        pad_masks.append(lang_masks)

        # full attention between image and language inputs
        num_lang_embs = lang_emb.shape[1]
        att_masks += [0] * num_lang_embs
```

* `self.paligemma_with_expert.embed_language_tokens(lang_tokens)`ï¼šè°ƒç”¨`PaliGemma`çš„â€œè¯­è¨€å¤§è„‘â€ï¼ŒæŠŠè¯­è¨€æŒ‡ä»¤ï¼ˆä¸€ä¸²token IDï¼‰è½¬æ¢æˆåµŒå…¥å‘é‡ã€‚
* `embs.append(lang_emb)`ï¼šæŠŠè¯­è¨€åµŒå…¥å‘é‡ï¼ˆæ¯”å¦‚10ä¸ªï¼‰åŠ å…¥åˆ—è¡¨ã€‚
* `pad_masks.append(lang_masks)`ï¼šè¯­è¨€æŒ‡ä»¤æœ‰é•¿æœ‰çŸ­ï¼Œè¿™é‡Œä½¿ç”¨å®ƒè‡ªå¸¦çš„å¡«å……æ©ç ã€‚
* `att_masks += [0] * num_lang_embs`ï¼šå†æ¬¡å…³é”®ï¼æˆ‘ä»¬åˆä¸ºè¯­è¨€tokenæ·»åŠ äº† `num_lang_embs` ä¸ª (æ¯”å¦‚10ä¸ª) `0`ã€‚

**ç»„åˆ**

```
        embs = torch.cat(embs, dim=1)
        pad_masks = torch.cat(pad_masks, dim=1)
        att_masks = torch.tensor(att_masks, dtype=torch.bool, device=pad_masks.device)
        ...
        att_masks = att_masks[None, :].expand(bsize, len(att_masks))

        return embs, pad_masks, att_masks
```

* `torch.cat(...)`ï¼šæŠŠæ‰€æœ‰å›¾åƒå‘é‡å’Œè¯­è¨€å‘é‡æ‹¼æ¥ï¼ˆconcatenateï¼‰æˆä¸€ä¸ªè¶…é•¿çš„åºåˆ—ã€‚
* `att_masks` çš„ç»“æœï¼š
  * å‡è®¾æœ‰1å¼ å›¾ (576ä¸ªtoken) å’Œ1æ¡æŒ‡ä»¤ (10ä¸ªtoken)ã€‚
  * `att_masks` åˆ—è¡¨ä¼šæ˜¯ï¼š`[0, 0, ..., 0 (å…±576ä¸ª), 0, 0, ..., 0 (å…±10ä¸ª)]`ã€‚
  * å½“ `make_att_2d_masks` å¯¹å®ƒè¿›è¡Œ `cumsum` æ—¶ï¼Œå¾—åˆ°çš„â€œå—IDâ€å‘é‡ä¼šæ˜¯ï¼š`[0, 0, 0, ..., 0, 0, 0]`ã€‚
* ç»“è®ºï¼šæ‰€æœ‰å‰ç¼€tokenï¼ˆå›¾åƒ+è¯­è¨€ï¼‰çš„â€œå—IDâ€éƒ½æ˜¯ `0`ã€‚æ ¹æ® `make_att_2d_masks` çš„é€»è¾‘ (`ID_k <= ID_j`)ï¼Œ`0 <= 0` æ°¸è¿œä¸º `True`ã€‚è¿™æ„å‘³ç€æ‰€æœ‰å›¾åƒå’Œè¯­è¨€tokenä¹‹é—´å¯ä»¥ç›¸äº’å®Œå…¨å…³æ³¨ï¼ˆFull Attentionï¼‰ã€‚

æ€»ç»“ï¼š`embed_prefix` æŠŠå›¾åƒå’Œè¯­è¨€â€œæ‰“åŒ…â€æˆä¸€ä¸ªæ•´ä½“ï¼Œå¹¶è®¾ç½®å¥½äº†æ³¨æ„åŠ›è§„åˆ™ï¼Œå…è®¸å®ƒä»¬å†…éƒ¨è‡ªç”±åœ°äº¤æµä¿¡æ¯ã€‚

***

å¥½çš„ï¼Œæˆ‘ä»¬æ¥ç€çœ‹ `embed_suffix`ã€‚

â€œå­¦å¾’â€çœ‹å®Œäº†èœè°±ï¼ˆ`embed_prefix`ï¼‰ï¼Œç°åœ¨ä»–è¦å¼€å§‹å¤„ç†\*\*æ‰‹å¤´çš„â€œæ´»å„¿â€\*\*äº†ã€‚

#### 2. `embed_suffix`ï¼šç†è§£â€œåšâ€å’Œâ€œæ—¶é—´â€

è¿™ä¸ªå‡½æ•°çš„å”¯ä¸€ç›®æ ‡ï¼šæŠŠæœºå™¨äººçŠ¶æ€ï¼ˆ`state`ï¼‰ã€å¸¦å™ªåŠ¨ä½œï¼ˆ`noisy_actions`ï¼‰å’Œå½“å‰æ—¶é—´ï¼ˆ`timestep`ï¼‰è½¬æ¢æˆå¦ä¸€ç»„åµŒå…¥å‘é‡ã€‚

è¿™é‡Œæ˜¯ `Pi0` å’Œ `Pi0.5` å˜ä½“å·®å¼‚æœ€å¤§çš„åœ°æ–¹ã€‚

```
    def embed_suffix(self, state, noisy_actions, timestep):
        embs = []
        pad_masks = []
        att_masks = []
```

* è§£é‡Šï¼šåŒæ ·ï¼Œåˆå§‹åŒ–ä¸‰ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨æ¥æ”¶é›†æ‰€æœ‰åç¼€ï¼ˆçŠ¶æ€ã€åŠ¨ä½œï¼‰çš„ä¿¡æ¯ã€‚

**(A) Pi0 æ¨¡å¼ï¼šå¤„ç†â€œçŠ¶æ€â€**

```
        if not self.pi05:  # <-- å¦‚æœæ˜¯ Pi0 (pi05=False)
            ...
            # Embed state
            def state_proj_func(state):
                return self.state_proj(state)

            state_emb = self._apply_checkpoint(state_proj_func, state)

            embs.append(state_emb[:, None, :])
            bsize = state_emb.shape[0]
            device = state_emb.device

            state_mask = torch.ones(bsize, 1, dtype=torch.bool, device=device)
            pad_masks.append(state_mask)
```

* `if not self.pi05`ï¼šè¿™ä¸ªä»£ç å—åªæœ‰åœ¨ Pi0 æ¨¡å¼ä¸‹æ‰ä¼šæ‰§è¡Œã€‚Pi0.5 æ¨¡å¼ä¼šå®Œå…¨è·³è¿‡è¿™ä¸€æ­¥ï¼ˆå®ƒä¸ä½¿ç”¨ `state` ä½œä¸ºtokenï¼‰ã€‚
* `self.state_proj(state)`ï¼šè°ƒç”¨ `__init__` ä¸­å®šä¹‰çš„é‚£ä¸ªçº¿æ€§å±‚ï¼ŒæŠŠ `32` ç»´çš„ `state` å‘é‡\*\*æŠ•å½±ï¼ˆæ”¾å¤§ï¼‰\*\*åˆ°å’Œæ¨¡å‹ä¸€æ ·çš„ `width` ç»´åº¦ (æ¯”å¦‚ `2048`)ã€‚
* `embs.append(state_emb[:, None, :])`ï¼š`state_emb` å½¢çŠ¶æ˜¯ `(B, D)`ï¼Œ`[:, None, :]` æŠŠå®ƒå˜æˆ `(B, 1, D)`ï¼Œå³ä¸€ä¸ªé•¿åº¦ä¸º1çš„â€œtokenåºåˆ—â€ã€‚
* `pad_masks.append(...)`ï¼šä¸ºè¿™ä¸ªâ€œçŠ¶æ€tokenâ€æ·»åŠ  `True`ï¼Œè¡¨ç¤ºå®ƒæ˜¯çœŸå®çš„ã€‚

```
            # Set attention masks so that image and language inputs do not attend to state or actions
            att_masks += [1]
```

* `att_masks += [1]`ï¼šæå…¶å…³é”®ï¼æˆ‘ä»¬åœ¨ `att_masks` åˆ—è¡¨é‡ŒåŠ äº†ç¬¬ä¸€ä¸ª `1`ã€‚
* å›å¿† `embed_prefix`ï¼šå‰ç¼€ï¼ˆå›¾åƒ+è¯­è¨€ï¼‰çš„ `att_masks` å…¨æ˜¯ `0`ã€‚
* `cumsum` ç´¯ç§¯å’Œï¼š
  * `att_masks` åˆ—è¡¨ç°åœ¨æ˜¯ `[0, 0, ..., 0, 1]`ã€‚
  * å¯¹åº”çš„â€œå—IDâ€ä¼šæ˜¯ `[0, 0, ..., 0, 1]`ã€‚
* å«ä¹‰ï¼š
  * å‰ç¼€ (ID 0) -> åç¼€ (ID 1)ï¼š`1 <= 0` ä¸º `False`ã€‚å‰ç¼€ï¼ˆèœè°±ï¼‰ä¸èƒ½â€œå·çœ‹â€åç¼€ï¼ˆåŠ¨ä½œï¼‰ã€‚è¿™ç¬¦åˆé€»è¾‘ã€‚
  * åç¼€ (ID 1) -> å‰ç¼€ (ID 0)ï¼š`0 <= 1` ä¸º `True`ã€‚åç¼€ï¼ˆåŠ¨ä½œï¼‰å¯ä»¥â€œå›å¤´çœ‹â€å‰ç¼€ï¼ˆèœè°±ï¼‰ã€‚è¿™ä¹Ÿç¬¦åˆé€»è¾‘ã€‚

**(B) æ‰€æœ‰æ¨¡å¼ï¼šå¤„ç†â€œæ—¶é—´â€å’Œâ€œåŠ¨ä½œâ€**

```
        # Embed timestep using sine-cosine positional encoding...
        time_emb = create_sinusoidal_pos_embedding(
            timestep, self.action_in_proj.out_features, min_period=4e-3, max_period=4.0, ...
        )
        time_emb = time_emb.type(dtype=timestep.dtype)
```

* è§£é‡Šï¼šè°ƒç”¨æˆ‘ä»¬ä¹‹å‰è¯¦ç»†åˆ†æè¿‡çš„ `create_sinusoidal_pos_embedding` å‡½æ•°ã€‚
* è¾“å…¥ï¼š`timestep` (ä¸€ä¸ª `(B,)` çš„æ—¶é—´å‘é‡ï¼Œæ¯”å¦‚ `[0.7, 0.7, 0.7, ...]`) å’Œ `self.action_in_proj.out_features` (å³ `width`ï¼Œæ¯”å¦‚ `2048`)ã€‚
* è¾“å‡ºï¼š`time_emb` æ˜¯ä¸€ä¸ª `(B, 2048)` çš„æ—¶é—´åµŒå…¥å‘é‡ã€‚

```
        # Fuse timestep + action information using an MLP
        def action_proj_func(noisy_actions):
            return self.action_in_proj(noisy_actions)

        action_emb = self._apply_checkpoint(action_proj_func, noisy_actions)
```

* `self.action_in_proj(noisy_actions)`ï¼šè°ƒç”¨è¾“å…¥æŠ•å½±å±‚ã€‚
* è¾“å…¥ï¼š`noisy_actions` (å½¢çŠ¶ `(B, 16, 32)`)ï¼Œ`B` æ˜¯æ‰¹é‡ï¼Œ`16` æ˜¯åŠ¨ä½œåºåˆ—é•¿åº¦ï¼ˆ`action_horizon`ï¼‰ï¼Œ`32` æ˜¯åŠ¨ä½œç»´åº¦ã€‚
* è¾“å‡ºï¼š`action_emb` (å½¢çŠ¶ `(B, 16, 2048)`)ã€‚

**(C) Pi0 vs Pi0.5ï¼šèåˆæ–¹å¼**

ç°åœ¨ï¼Œæ¨¡å‹æœ‰äº† `action_emb` (å½¢çŠ¶ `(B, 16, 2048)`) å’Œ `time_emb` (å½¢çŠ¶ `(B, 2048)`)ã€‚å®ƒå¦‚ä½•å°†ä¸¤è€…ç»“åˆï¼Ÿ

Python

```
        if not self.pi05:  # <-- Pi0 æ¨¡å¼
            time_emb = time_emb[:, None, :].expand_as(action_emb)
            action_time_emb = torch.cat([action_emb, time_emb], dim=2)

            # Apply MLP layers
            def mlp_func(action_time_emb):
                x = self.action_time_mlp_in(action_time_emb)
                ...
                return self.action_time_mlp_out(x)
            
            action_time_emb = self._apply_checkpoint(mlp_func, action_time_emb)
            adarms_cond = None
```

* Pi0 æ¨¡å¼ (æ‹¼æ¥èåˆ)ï¼š
  1. `time_emb.expand_as(action_emb)`ï¼šæŠŠ `(B, 2048)` çš„ `time_emb` å¤åˆ¶16æ¬¡ï¼Œå˜æˆ `(B, 16, 2048)`ã€‚
  2. `torch.cat(...)`ï¼šåœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆ`dim=2`ï¼‰ä¸Šæ‹¼æ¥ã€‚`action_emb` (2048ç»´) å’Œ `time_emb` (2048ç»´) æ‹¼æˆäº† `(B, 16, 4096)`ã€‚
  3. `self.action_time_mlp_in(action_time_emb)`ï¼šè¿™ä¸ªMLPå±‚æŠŠ `4096` ç»´å‹ç¼©å› `2048` ç»´ã€‚
  4. ç»“æœï¼šå¾—åˆ°ä¸€ä¸ª `(B, 16, 2048)` çš„ `action_time_emb` å‘é‡ï¼Œå®ƒåŒæ—¶åŒ…å«äº†åŠ¨ä½œå’Œæ—¶é—´ä¿¡æ¯ã€‚
  5. `adarms_cond = None`ï¼šPi0 ä¸ä½¿ç”¨ AdaRMSã€‚

Python

```
        else:  # <-- Pi0.5 æ¨¡å¼
            # time MLP (for adaRMS)
            def time_mlp_func(time_emb):
                ...
                return F.silu(x)

            time_emb = self._apply_checkpoint(time_mlp_func, time_emb)
            action_time_emb = action_emb
            adarms_cond = time_emb
```

* Pi0.5 æ¨¡å¼ (AdaRMS è°ƒèŠ‚)ï¼š
  1. `time_mlp_func(time_emb)`ï¼š`time_emb` (å½¢çŠ¶ `(B, 2048)`) è¢«é€å…¥å®ƒè‡ªå·±çš„ `time_mlp`ã€‚
  2. `action_time_emb = action_emb`ï¼šæ³¨æ„ï¼ `action_time_emb` å°±æ˜¯ `action_emb`ï¼æ—¶é—´ä¿¡æ¯æ ¹æœ¬æ²¡æœ‰è¢«æ‹¼æ¥è¿›å»ï¼
  3. `adarms_cond = time_emb`ï¼šå¤„ç†è¿‡çš„ `time_emb` è¢«å•ç‹¬å­˜æ”¾åœ¨ `adarms_cond` å˜é‡é‡Œã€‚
  4. ç»“æœï¼š`action_time_emb` (å½¢çŠ¶ `(B, 16, 2048)`) åªåŒ…å«åŠ¨ä½œä¿¡æ¯ã€‚`adarms_cond` (å½¢çŠ¶ `(B, 2048)`) åªåŒ…å«æ—¶é—´ä¿¡æ¯ã€‚è¿™ä¸ª `adarms_cond` ä¼šåœ¨ `forward` å‡½æ•°ä¸­è¢«ä¼ é€’ç»™ `PaliGemmaWithExpertModel`ï¼Œç”¨äºåŠ¨æ€è°ƒèŠ‚ `ActionExpert` æ¨¡å‹å†…éƒ¨çš„ `RMSNorm` å±‚ã€‚

**(D) ç»„åˆåç¼€**

Python

```
        # Add to input tokens
        embs.append(action_time_emb)
        ...
        pad_masks.append(action_time_mask)

        # Set attention masks ...
        att_masks += [1] + ([0] * (self.config.action_horizon - 1))
```

* `embs.append(action_time_emb)`ï¼šæŠŠèåˆåçš„ `(B, 16, 2048)` å‘é‡ï¼ˆ`action_time_emb`ï¼‰åŠ å…¥åˆ—è¡¨ã€‚
* `att_masks += [1] + ([0] * 15)`ï¼šè¿™æ˜¯ç¬¬äºŒä¸ªå…³é”®ã€‚
  * æˆ‘ä»¬åœ¨ `att_masks` åˆ—è¡¨é‡ŒåŠ äº†ä¸€ä¸ª `1` å’Œ `15` ä¸ª `0`ã€‚

**æœ€ç»ˆçš„ `att_masks` ç»“æ„ (Pi0 æ¨¡å¼)**

æˆ‘ä»¬æ¥æ€»ç»“ä¸€ä¸‹ `att_masks` åˆ—è¡¨ç°åœ¨çš„æ ·å­ (å‡è®¾1ä¸ªå›¾åƒ=576, è¯­è¨€=10, çŠ¶æ€=1, åŠ¨ä½œ=16)ï¼š

* å‰ç¼€ (Prefix): `[0, 0, ..., 0]` (å…± 576+10 = 586 ä¸ª `0`)
* åç¼€ (Suffix):
  * çŠ¶æ€ (State): `[1]` (æ¥è‡ª Pi0 æ¨¡å¼ `if not self.pi05`)
  * åŠ¨ä½œ (Action): `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]` (æ¥è‡ª `[1] + ([0] * 15)`)

att\_masks å®Œæ•´åˆ—è¡¨ï¼š

\[0...0, 1, 1, 0...0]

(586ä¸ª0, æ¥ç€1, æ¥ç€1, æ¥ç€15ä¸ª0)

cumsum åçš„â€œå—IDâ€ï¼š

\[0...0, 1, 2, 2...2]

* å‰ç¼€ (å›¾åƒ+è¯­è¨€): å—ID = `0`
* çŠ¶æ€ (State): å—ID = `1`
* åŠ¨ä½œ (Action):
  * ç¬¬1ä¸ªåŠ¨ä½œtoken: å—ID = `2`
  * ç¬¬2-16ä¸ªåŠ¨ä½œtoken: å—ID = `2`

è¿™å®ç°äº†ä»€ä¹ˆæ³¨æ„åŠ›ï¼Ÿ

1. å—0 (å‰ç¼€): åªèƒ½çœ‹å—0 (ID `0 <= 0`)ã€‚
2. å—1 (çŠ¶æ€): èƒ½çœ‹å—0å’Œå—1 (ID `0 <= 1`, `1 <= 1`)ã€‚
3. å—2 (åŠ¨ä½œ): èƒ½çœ‹å—0ã€å—1å’Œå—2 (ID `0 <= 2`, `1 <= 2`, `2 <= 2`)ã€‚

ç»“è®ºï¼š`embed_prefix` å’Œ `embed_suffix` å·§å¦™åœ°åˆ©ç”¨ `0` å’Œ `1`ï¼Œä¸ `make_att_2d_masks` é…åˆï¼Œæ„å»ºäº†ä¸€ä¸ªå®Œç¾çš„â€œå‰ç¼€-LMï¼ˆPrefix-LMï¼‰â€æ³¨æ„åŠ›æ©ç ï¼Œå®ƒå¼ºåˆ¶æ¨¡å‹ï¼š

> åŠ¨ä½œï¼ˆå’ŒçŠ¶æ€ï¼‰å¯ä»¥å…³æ³¨ï¼ˆattend toï¼‰å›¾åƒå’Œè¯­è¨€ï¼ˆèœè°±ï¼‰ï¼Œ
>
> ä½†åè¿‡æ¥ï¼Œ
>
> å›¾åƒå’Œè¯­è¨€ï¼ˆèœè°±ï¼‰æ°¸è¿œä¸èƒ½â€œå·çœ‹â€æœªæ¥çš„åŠ¨ä½œã€‚

è¿™å°±æ˜¯è¿™ä¸¤ä¸ªåµŒå…¥å‡½æ•°çš„æ ¸å¿ƒåŠŸèƒ½ã€‚

å¥½çš„ï¼Œæˆ‘ä»¬å·²ç»ä¸‡äº‹ä¿±å¤‡äº†ã€‚

æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†ï¼š

1. `prefix_embs` (å‰ç¼€åµŒå…¥)ï¼šæ¥è‡ª `embed_prefix`ï¼ŒåŒ…å«äº†ï¼ˆå›¾åƒ+è¯­è¨€ï¼‰ã€‚
2. `suffix_embs` (åç¼€åµŒå…¥)ï¼šæ¥è‡ª `embed_suffix`ï¼ŒåŒ…å«äº†ï¼ˆçŠ¶æ€+å¸¦å™ªåŠ¨ä½œ $$ $x_t$ $$+æ—¶é—´ $$ $t$ $$ï¼‰ã€‚
3. `pad_masks` (å¡«å……æ©ç )ï¼šä¸€ä¸ªé•¿ `(B, N)` å‘é‡ï¼Œæ ‡è®°å“ªäº›æ˜¯çœŸå®tokenã€‚
4. `att_masks` (æ³¨æ„åŠ›æ©ç )ï¼šä¸€ä¸ªé•¿ `(B, N)` å‘é‡ï¼Œç”¨äºå®šä¹‰â€œå—IDâ€ï¼ˆæ¯”å¦‚ `[0,0,0, 1, 2,2,2]`ï¼‰ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬è¿›å…¥è®­ç»ƒçš„æ ¸å¿ƒï¼š`forward` æ–¹æ³•çš„ä¸»ä½“éƒ¨åˆ†ã€‚

#### 3. `forward` æ–¹æ³•ï¼šæ‰§è¡Œâ€œçœ‹é¢˜-é¢„æµ‹-æ‰“åˆ†â€

`forward` æ–¹æ³•çš„ç›®æ ‡æ˜¯è®¡ç®—æŸå¤±ï¼ˆlossï¼‰ã€‚

Python

```
    def forward(self, observation, actions, noise=None, time=None) -> Tensor:
        """Do a full training forward pass and compute the loss..."""
```

* è§£é‡Šï¼šå®ƒæ¥æ”¶ `observation`ï¼ˆè§‚æµ‹ï¼‰å’Œ `actions`ï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰ã€‚

Python

```
        images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=True)

        if noise is None:
            noise = self.sample_noise(actions.shape, actions.device)

        if time is None:
            time = self.sample_time(actions.shape[0], actions.device)
```

* è§£é‡Šï¼š
  1. `_preprocess_observation`ï¼šåªæ˜¯ä¸ªè¾…åŠ©å‡½æ•°ï¼ŒæŠŠæ•°æ®æ•´ç†æˆå¼ é‡ã€‚
  2. `sample_noise`ï¼šç”Ÿæˆä¸€ä¸ªå’Œ `actions` å½¢çŠ¶ç›¸åŒçš„éšæœºé«˜æ–¯å™ªå£° $$ $\epsilon$ $$ã€‚
  3. `sample_time`ï¼šè°ƒç”¨æˆ‘ä»¬åˆ†æè¿‡çš„ `sample_beta`ï¼Œç”Ÿæˆä¸€ä¸ª $$ $\alpha=1.5, \beta=1$ $$ çš„ã€åå‘1.0çš„éšæœºæ—¶é—´ $$ $t$ $$ã€‚

Python

```
        time_expanded = time[:, None, None]
        x_t = time_expanded * noise + (1 - time_expanded) * actions
        u_t = noise - actions
```

* è§£é‡Šï¼šæµåŒ¹é…ï¼ˆFlow Matchingï¼‰çš„æ ¸å¿ƒï¼
  1. `time_expanded`ï¼šæŠŠ `(B,)` çš„æ—¶é—´ $$t$$ æ‰©å±•æˆ `(B, 1, 1)`ï¼Œä»¥ä¾¿å’Œ `(B, 16, 32)` çš„åŠ¨ä½œè¿›è¡Œå¹¿æ’­è¿ç®—ã€‚
  2. `x_t = ...`ï¼šè¿™å°±æ˜¯åœ¨åˆ¶ä½œâ€œç»ƒä¹ é¢˜â€ã€‚å®ƒåœ¨â€œå®Œç¾åŠ¨ä½œâ€ `actions` (åœ¨ $$ $t=0$ $$) å’Œâ€œçº¯å™ªå£°â€ `noise` (åœ¨ $$ $t=1$ $$) ä¹‹é—´è¿›è¡Œçº¿æ€§æ’å€¼ã€‚
  3. `u_t = noise - actions`ï¼šè¿™å°±æ˜¯â€œæ ‡å‡†ç­”æ¡ˆâ€ã€‚$$ $u_t$ $$ æ˜¯æ¨¡å‹éœ€è¦é¢„æµ‹çš„ç›®æ ‡â€œé€Ÿåº¦å‘é‡â€ã€‚

Python

```
        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(images, img_masks, lang_tokens, lang_masks)
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = self.embed_suffix(state, x_t, time)
```

* è§£é‡Šï¼šè°ƒç”¨æˆ‘ä»¬åˆšåˆšè¯¦ç»†åˆ†æè¿‡çš„é‚£ä¸¤ä¸ªå‡½æ•°ã€‚
  * `embed_prefix` å¤„ç†ï¼ˆå›¾åƒ+è¯­è¨€ï¼‰ã€‚
  * `embed_suffix` å¤„ç†ï¼ˆçŠ¶æ€, å¸¦å™ªåŠ¨ä½œ $$ $x_t$ $$, æ—¶é—´ $$ $t$ $$ï¼‰ã€‚
  * æ³¨æ„ `x_t` (ç»ƒä¹ é¢˜) è¢«ä¼ è¿›å»äº†ï¼Œè€Œä¸æ˜¯ `actions` (æ ‡å‡†ç­”æ¡ˆ)ã€‚
  * `adarms_cond`ï¼šåœ¨ Pi0.5 æ¨¡å¼ä¸‹ï¼Œè¿™é‡Œä¼šæ¥æ”¶åˆ°å¤„ç†è¿‡çš„æ—¶é—´åµŒå…¥ï¼›åœ¨ Pi0 æ¨¡å¼ä¸‹ï¼Œè¿™é‡Œæ˜¯ `None`ã€‚

Python

```
        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)
        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)

        att_2d_masks = make_att_2d_masks(pad_masks, att_masks)
        position_ids = torch.cumsum(pad_masks, dim=1) - 1
```

* è§£é‡Šï¼š
  1. `torch.cat`ï¼šæŠŠå‰ç¼€å’Œåç¼€çš„æ‰€æœ‰æ©ç æ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„åºåˆ—ã€‚
  2. `make_att_2d_masks(...)`ï¼šè°ƒç”¨æˆ‘ä»¬åˆ†æè¿‡çš„å‡½æ•°ï¼Œæ ¹æ®æ‹¼æ¥åçš„ `pad_masks` å’Œ `att_masks`ï¼ˆé‚£ä¸ª `[0,0,1,2,2]`ï¼‰æ¥ç”Ÿæˆæœ€ç»ˆçš„ `(B, N, N)` äºŒç»´æ³¨æ„åŠ›çŸ©é˜µã€‚
  3. `position_ids`ï¼šè®¡ç®—æ¯ä¸ªtokençš„ï¼ˆéå¡«å……ï¼‰ä½ç½®ç´¢å¼•ã€‚

Python

```
        # Prepare attention masks
        att_2d_masks_4d = self._prepare_attention_masks_4d(att_2d_masks)
```

* è§£é‡Šï¼š`_prepare_attention_masks_4d` æ˜¯ä¸ªè¾…åŠ©å‡½æ•°ã€‚å®ƒæŠŠ `True/False` çš„æ©ç çŸ©é˜µè½¬æ¢æˆ Transformer (Gemma) æœŸæœ›çš„ `0.0` å’Œ è´Ÿæ— ç©·ï¼ˆ`-2.38e38`ï¼‰çš„æµ®ç‚¹æ•°çŸ©é˜µã€‚

Python

```
        # Apply gradient checkpointing if enabled
        def forward_func(prefix_embs, suffix_embs, att_2d_masks_4d, position_ids, adarms_cond):
            (_, suffix_out), _ = self.paligemma_with_expert.forward(
                attention_mask=att_2d_masks_4d,
                position_ids=position_ids,
                past_key_values=None,
                inputs_embeds=[prefix_embs, suffix_embs],
                use_cache=False,
                adarms_cond=[None, adarms_cond],
            )
            return suffix_out
```

* è§£é‡Šï¼š
  1. è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ª `forward_func`ï¼Œå®ƒå°è£…äº†å¯¹æ¨¡å‹æ ¸å¿ƒ `self.paligemma_with_expert.forward` çš„è°ƒç”¨ã€‚
  2. `attention_mask=att_2d_masks_4d`ï¼šä¼ å…¥æˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œçš„å‰ç¼€-LM æ©ç ã€‚
  3. `inputs_embeds=[prefix_embs, suffix_embs]`ï¼šåŒæ—¶ä¼ å…¥å‰ç¼€å’Œåç¼€çš„åµŒå…¥ã€‚æ¨¡å‹å†…éƒ¨ï¼ˆ`PaliGemmaWithExpertModel`ï¼‰ä¼šåˆ†åˆ«æŠŠå®ƒä»¬äº¤ç»™ `PaliGemma`ï¼ˆè§†è§‰å¤§è„‘ï¼‰å’Œ `ActionExpert`ï¼ˆåŠ¨ä½œå¤§è„‘ï¼‰å¤„ç†ã€‚
  4. `adarms_cond=[None, adarms_cond]`ï¼š
     * ç¬¬ä¸€ä¸ª `None` å¯¹åº” `PaliGemma`ï¼ˆå®ƒä¸ç”¨ AdaRMSï¼‰ã€‚
     * ç¬¬äºŒä¸ª `adarms_cond` å¯¹åº” `ActionExpert`ã€‚åœ¨ Pi0.5 æ¨¡å¼ä¸‹ï¼Œè¿™é‡Œä¼ å…¥äº†æ—¶é—´åµŒå…¥ï¼›åœ¨ Pi0 æ¨¡å¼ä¸‹ï¼Œè¿™é‡Œä¼ å…¥äº† `None`ã€‚
  5. `(_, suffix_out), _`ï¼šæ¨¡å‹ä¼šè¿”å› `(prefix_output, suffix_output)`ã€‚æˆ‘ä»¬åªå…³å¿ƒ `suffix_out`ï¼ˆåç¼€çš„è¾“å‡ºï¼‰ï¼Œå› ä¸ºè¿™æ‰æ˜¯â€œåŠ¨ä½œå¤§è„‘â€çš„æ€è€ƒç»“æœã€‚

Python

```
        suffix_out = self._apply_checkpoint(
            forward_func, prefix_embs, suffix_embs, att_2d_masks_4d, position_ids, adarms_cond
        )
```

* è§£é‡Šï¼šæ‰§è¡Œä¸Šé¢å®šä¹‰çš„ `forward_func`ã€‚

Python

```
        suffix_out = suffix_out[:, -self.config.action_horizon :]
        suffix_out = suffix_out.to(dtype=torch.float32)
```

* è§£é‡Šï¼š`suffix_out` åŒ…å«äº†ï¼ˆçŠ¶æ€+åŠ¨ä½œï¼‰çš„è¾“å‡ºã€‚æˆ‘ä»¬åªå…³å¿ƒæœ€å `action_horizon` ä¸ªï¼ˆæ¯”å¦‚16ä¸ªï¼‰tokenï¼Œå› ä¸ºå®ƒä»¬å¯¹åº”çš„æ˜¯åŠ¨ä½œã€‚

Python

```
        # Apply gradient checkpointing to final action projection if enabled
        def action_out_proj_func(suffix_out):
            return self.action_out_proj(suffix_out)

        v_t = self._apply_checkpoint(action_out_proj_func, suffix_out)
```

* è§£é‡Šï¼š
  1. `self.action_out_proj`ï¼šè°ƒç”¨ `__init__` ä¸­å®šä¹‰çš„è¾“å‡ºæŠ•å½±å±‚ã€‚
  2. å®ƒæŠŠ `suffix_out`ï¼ˆå½¢çŠ¶ `(B, 16, 2048)`ï¼‰å‹ç¼©å› `(B, 16, 32)`ã€‚
  3. `v_t`ï¼šè¿™å°±æ˜¯å­¦å¾’ï¼ˆæ¨¡å‹ï¼‰å¯¹â€œä¿®æ­£æ–¹å‘â€çš„æœ€ç»ˆé¢„æµ‹ã€‚

Python

```
        return F.mse_loss(u_t, v_t, reduction="none")
```

* è§£é‡Šï¼šæœ€åä¸€æ­¥ï¼šæ‰“åˆ†ï¼
* è®¡ç®—æ ‡å‡†ç­”æ¡ˆ `u_t`ï¼ˆ`noise - actions`ï¼‰å’Œæ¨¡å‹é¢„æµ‹ `v_t` ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚
* è¿™ä¸ª `loss` ä¼šè¢«è¿”å›ï¼ŒPyTorch çš„ä¼˜åŒ–å™¨ä¼šç”¨å®ƒæ¥æ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼ˆåå‘ä¼ æ’­ï¼‰ã€‚

***

`forward` (è®­ç»ƒ) çš„æµç¨‹å°±æ˜¯è¿™æ ·ã€‚

å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æ¥çœ‹ `sample_actions` (æ¨ç†)ã€‚

è¿™æ˜¯å­¦å¾’çœŸæ­£â€œä¸Šå²—åšèœâ€çš„è¿‡ç¨‹ã€‚è¿™ä¸ªæ–¹æ³•æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ `actions`ï¼Œå®ƒçš„ç›®æ ‡æ˜¯ä»é›¶ï¼ˆå™ªå£°ï¼‰å¼€å§‹ï¼Œç”ŸæˆåŠ¨ä½œã€‚

è¿™ä¸ªè¿‡ç¨‹è¦å·§å¦™å¾—å¤šï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªè¿­ä»£å¾ªç¯ï¼Œå¹¶ä¸”ç”¨åˆ°äº†ä¸€ä¸ªå…³é”®çš„æ€§èƒ½ä¼˜åŒ–ï¼šKV ç¼“å­˜ï¼ˆKV Cacheï¼‰ã€‚

#### 4. `sample_actions` (æ¨ç†): â€œè¿­ä»£åšèœâ€

Python

```
    @torch.no_grad()
    def sample_actions(self, device, observation, noise=None, num_steps=10) -> Tensor:
        """Do a full inference forward and compute the action ..."""
```

* `@torch.no_grad()`ï¼šè¿™æ˜¯ä¸€ä¸ª PyTorch è£…é¥°å™¨ï¼Œå®ƒå‘Šè¯‰ PyTorchï¼šâ€œåœ¨è¿™ä¸ªå‡½æ•°é‡Œï¼Œä¸è¦è®¡ç®—æ¢¯åº¦â€ã€‚è¿™èƒ½æå¤§åœ°èŠ‚çœæ˜¾å­˜å’Œè®¡ç®—èµ„æºï¼Œå› ä¸ºåœ¨æ¨ç†æ—¶æˆ‘ä»¬ä¸éœ€è¦åå‘ä¼ æ’­ã€‚
* è¾“å…¥ï¼šåªæ¥æ”¶ `observation`ï¼ˆè§‚æµ‹ï¼‰ã€‚`num_steps`ï¼ˆæ¯”å¦‚10ï¼‰å‘Šè¯‰æ¨¡å‹è¦è¿­ä»£å¤šå°‘æ­¥æ¥å»å™ªã€‚

Python

```
        bsize = observation.state.shape[0]
        if noise is None:
            actions_shape = (bsize, self.config.action_horizon, self.config.action_dim)
            noise = self.sample_noise(actions_shape, device)
```

* è§£é‡Šï¼š
  1. è·å–æ‰¹é‡å¤§å° `bsize`ã€‚
  2. å¦‚æœå¤–éƒ¨æ²¡æœ‰æä¾› `noise`ï¼Œå®ƒå°±è‡ªå·±ç”Ÿæˆä¸€ä¸ªã€‚
  3. `noise` åœ¨è¿™é‡Œå°±æ˜¯ $$x_t$$ çš„åˆå§‹å€¼ï¼ˆåœ¨ $$t=1.0$$ æ—¶ï¼‰ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¼€å§‹â€œåšèœâ€çš„â€œçº¯é¢å›¢â€ã€‚

Python

```
        images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)

        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(images, img_masks, lang_tokens, lang_masks)
        prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks, prefix_att_masks)
        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1
```

* è§£é‡Šï¼šå’Œ `forward` (è®­ç»ƒ) ä¸€æ ·ï¼Œå®ƒé¦–å…ˆé¢„å¤„ç†è§‚æµ‹æ•°æ®ï¼Œå¹¶è°ƒç”¨ `embed_prefix` æ¥è·å–\*\*å‰ç¼€ï¼ˆå›¾åƒ+è¯­è¨€ï¼‰\*\*çš„åµŒå…¥å’Œæ©ç ã€‚

***

#### ğŸš€ å…³é”®ä¼˜åŒ–ï¼šKV ç¼“å­˜

æ¥ä¸‹æ¥çš„å‡ è¡Œæ˜¯æ¨ç†è¿‡ç¨‹ä¸­æœ€é‡è¦çš„ä¼˜åŒ–ã€‚

Python

```
        # Compute image and language key value cache
        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(prefix_att_2d_masks)
        ...

        _, past_key_values = self.paligemma_with_expert.forward(
            attention_mask=prefix_att_2d_masks_4d,
            position_ids=prefix_position_ids,
            past_key_values=None,
            inputs_embeds=[prefix_embs, None],  # <-- åªä¼ å…¥å‰ç¼€ï¼
            use_cache=True,                     # <-- å‘Šè¯‰æ¨¡å‹â€œè¯·è¿”å›ç¼“å­˜â€ï¼
        )
```

* æ€è€ƒä¸€ä¸‹ï¼šåœ¨ `num_steps=10` çš„è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œâ€œèœè°±â€ï¼ˆå›¾åƒ+è¯­è¨€ï¼‰æ˜¯æ°¸è¿œä¸ä¼šæ”¹å˜çš„ã€‚
* å¦‚æœæˆ‘ä»¬åœ¨æ¯ä¸€æ­¥è¿­ä»£ï¼ˆå…±10æ¬¡ï¼‰éƒ½è®© `PaliGemma`ï¼ˆè§†è§‰å¤§è„‘ï¼‰é‡æ–°çœ‹ä¸€éå›¾åƒå’Œè¯­è¨€ï¼Œé‚£å°†æ˜¯å·¨å¤§çš„æµªè´¹ã€‚
* è§£å†³æ–¹æ¡ˆï¼šæˆ‘ä»¬åœ¨è¿™é‡Œï¼ˆå¾ªç¯å¼€å§‹ä¹‹å‰ï¼‰åªè¿è¡Œä¸€æ¬¡ `PaliGemma`ã€‚
  1. `inputs_embeds=[prefix_embs, None]`ï¼šæˆ‘ä»¬\*\*åªæŠŠ `prefix_embs`ï¼ˆå‰ç¼€ï¼‰\*\*ä¼ ç»™æ¨¡å‹ã€‚`suffix_embs`ï¼ˆåç¼€ï¼‰éƒ¨åˆ†æ˜¯ `None`ã€‚
  2. `use_cache=True`ï¼šæˆ‘ä»¬å‘Šè¯‰æ¨¡å‹ï¼šâ€œè¯·è®¡ç®— `prefix_embs` çš„\*\*é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰\*\*å‘é‡ï¼Œå¹¶æŠŠå®ƒä»¬ä½œä¸º `past_key_values`ï¼ˆKVç¼“å­˜ï¼‰è¿”å›ç»™æˆ‘ã€‚â€
* `past_key_values`ï¼šè¿™å°±æ˜¯å­¦å¾’çš„é‚£å¼ â€œä¾¿ç­¾â€ã€‚å®ƒåŒ…å«äº† `PaliGemma` å¯¹â€œèœè°±â€çš„å…¨éƒ¨ç†è§£ï¼ˆæ‰€æœ‰å±‚çš„Kå’ŒVå‘é‡ï¼‰ã€‚

***

#### ğŸŒ€ è¿­ä»£å»å™ªå¾ªç¯

ç°åœ¨æˆ‘ä»¬æœ‰äº†â€œä¾¿ç­¾â€ï¼ˆ`past_key_values`ï¼‰å’Œâ€œé¢å›¢â€ï¼ˆ`noise`ï¼‰ï¼Œå¼€å§‹å¾ªç¯ã€‚

Python

```
        dt = -1.0 / num_steps
        dt = torch.tensor(dt, dtype=torch.float32, device=device)

        x_t = noise  # æˆ‘ä»¬çš„â€œé¢å›¢â€ï¼Œåˆå§‹æ˜¯çº¯å™ªå£°
        time = torch.tensor(1.0, dtype=torch.float32, device=device)
        while time >= -dt / 2: # å¾ªç¯ç›´åˆ° t çº¦ç­‰äº 0
            expanded_time = time.expand(bsize)
```

* è§£é‡Šï¼šè®¾ç½®æ—¶é—´æ­¥é•¿ `dt`ï¼ˆä¸€ä¸ªå°çš„è´Ÿæ•°ï¼Œæ¯”å¦‚ `-0.1`ï¼‰ã€‚`x_t` è¢«åˆå§‹åŒ–ä¸º `noise`ï¼Œ`time` è¢«åˆå§‹åŒ–ä¸º `1.0`ã€‚

Python

```
            v_t = self.denoise_step(
                state,
                prefix_pad_masks,
                past_key_values,  # <-- ä¼ å…¥â€œä¾¿ç­¾â€
                x_t,              # <-- ä¼ å…¥å½“å‰çš„â€œé¢å›¢â€
                expanded_time,
            )
```

* è§£é‡Šï¼šè¿™æ˜¯å¾ªç¯çš„æ ¸å¿ƒã€‚å®ƒè°ƒç”¨ä¸€ä¸ªè¾…åŠ©å‡½æ•° `denoise_step`ã€‚
* `denoise_step` çš„ä»»åŠ¡ï¼šæ‰§è¡Œä¸€æ¬¡é¢„æµ‹ã€‚å®ƒæ¥æ”¶â€œä¾¿ç­¾â€ï¼ˆ`past_key_values`ï¼‰ã€å½“å‰çš„â€œé¢å›¢â€ï¼ˆ`x_t`ï¼‰ã€`state` å’Œ `time`ï¼Œç„¶åè¿”å›å¯¹å½“å‰â€œä¿®æ­£æ–¹å‘â€ $$ $v_t$ $$ çš„é¢„æµ‹ã€‚
* (æˆ‘ä»¬ç¨åä¼šæ·±å…¥ `denoise_step` å‡½æ•°ã€‚)

Python

```
            # Euler step - use new tensor assignment instead of in-place operation
            x_t = x_t + dt * v_t
            time += dt
```

* è§£é‡Šï¼šæ¬§æ‹‰æ³•ï¼ˆEuler methodï¼‰ç§¯åˆ†ã€‚
  1. `x_t = x_t + dt * v_t`ï¼šâ€œä¿®æ­£é¢å›¢â€ã€‚æˆ‘ä»¬æ ¹æ®æ¨¡å‹é¢„æµ‹çš„æ–¹å‘ $$ $v_t$ $$ å’Œæ—¶é—´æ­¥é•¿ $$ $dt$ $$ï¼Œæ¥æ›´æ–°æˆ‘ä»¬çš„ $$ $x_t$ $$ã€‚
  2. `time += dt`ï¼šæ›´æ–°æ—¶é—´ $$ $t$ $$ï¼ˆæ¯”å¦‚ä» `1.0` å˜æˆ `0.9`ï¼‰ã€‚
* è¿™ä¸ªå¾ªç¯ä¼šé‡å¤ `num_steps` æ¬¡ï¼ˆæ¯”å¦‚10æ¬¡ï¼‰ã€‚

Python

```
        return x_t
```

* è§£é‡Šï¼šå½“ `time` çº¦ç­‰äº `0` æ—¶ï¼Œå¾ªç¯ç»“æŸã€‚æ­¤æ—¶çš„ `x_t`ï¼ˆå³ $$ $x_0$ $$ï¼‰å·²ç»ä»â€œçº¯å™ªå£°â€è¢«å®Œå…¨å»å™ªæˆäº†â€œå¹²å‡€çš„ã€å¯æ‰§è¡Œçš„åŠ¨ä½œâ€ã€‚å‡½æ•°å°†å…¶è¿”å›ã€‚

***

ç°åœ¨ï¼Œè¿™ä¸ªæ¨ç†æµç¨‹åº”è¯¥å¾ˆæ¸…æ¥šäº†ã€‚`sample_actions` è´Ÿè´£è®¾ç½®ï¼ˆKVç¼“å­˜ï¼‰å’Œå¾ªç¯ï¼ˆè¿­ä»£ï¼‰ï¼Œè€ŒçœŸæ­£çš„å·¥ä½œæ˜¯åœ¨ `denoise_step` é‡Œå®Œæˆçš„ã€‚

å¥½çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ `denoise_step` å‡½æ•°ã€‚

è¿™æ˜¯ `sample_actions` (æ¨ç†) å¾ªç¯å†…éƒ¨è°ƒç”¨çš„æ ¸å¿ƒè¾…åŠ©å‡½æ•°ã€‚

å®ƒçš„ç›®æ ‡éå¸¸æ˜ç¡®ï¼šç»™å®š $$ $t$ $$ æ—¶åˆ»çš„â€œé¢å›¢â€ï¼ˆ`x_t`ï¼‰å’Œâ€œä¾¿ç­¾â€ï¼ˆ`past_key_values`ï¼‰ï¼Œä»…æ‰§è¡Œä¸€æ­¥å»å™ªï¼Œå¹¶è¿”å›é¢„æµ‹çš„é€Ÿåº¦ $$ $v_t$ $$ã€‚

#### 5. `denoise_step` (æ¨ç†): â€œæŸ¥ä¾¿ç­¾ï¼Œä¿®é¢å›¢â€

Python

```
    def denoise_step(
        self,
        state,
        prefix_pad_masks,
        past_key_values,  # <-- â€œä¾¿ç­¾â€ (æ¥è‡ªPaliGemma)
        x_t,              # <-- å½“å‰çš„â€œé¢å›¢â€ (å¸¦å™ªåŠ¨ä½œ)
        timestep,         # <-- å½“å‰çš„æ—¶é—´ t
    ):
        """Apply one denoising step of the noise `x_t` at a given timestep."""
```

* è§£é‡Šï¼šæ¥æ”¶æ‰€æœ‰éœ€è¦çš„å½“å‰çŠ¶æ€ã€‚

Python

```
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = self.embed_suffix(state, x_t, timestep)
```

* è§£é‡Šï¼š
  1. å†æ¬¡è°ƒç”¨ `embed_suffix`ã€‚
  2. ä½†è¿™ä¸€æ¬¡ï¼Œå®ƒåµŒå…¥çš„æ˜¯æ¨ç†æ—¶çš„ $$ $state$ $$ã€å½“å‰çš„ $$ $x_t$ $$ å’Œå½“å‰çš„ $$ $timestep$ $$ã€‚
  3. è¿”å› `suffix_embs` (åç¼€åµŒå…¥) å’Œ `adarms_cond` (Pi0.5 æ¨¡å¼ä¸‹çš„æ—¶é—´åµŒå…¥)ã€‚

Python

```
        suffix_len = suffix_pad_masks.shape[1]
        batch_size = prefix_pad_masks.shape[0]
        prefix_len = prefix_pad_masks.shape[1]

        prefix_pad_2d_masks = prefix_pad_masks[:, None, :].expand(batch_size, suffix_len, prefix_len)

        suffix_att_2d_masks = make_att_2d_masks(suffix_pad_masks, suffix_att_masks)

        full_att_2d_masks = torch.cat([prefix_pad_2d_masks, suffix_att_2d_masks], dim=2)
```

* è§£é‡Šï¼šè¿™å‡ è¡Œåœ¨åŠ¨æ€åœ°åˆ›å»ºæ³¨æ„åŠ›æ©ç ã€‚
* æ€è€ƒä¸€ä¸‹ï¼šåœ¨ `forward` (è®­ç»ƒ) ä¸­ï¼Œå‰ç¼€å’Œåç¼€æ˜¯ä¸€èµ·è¢«é€å…¥æ¨¡å‹çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§åˆ›å»º `(N+M, N+M)` çš„å¤§æ©ç ã€‚
* ä½†åœ¨æ¨ç†æ—¶ï¼šå‰ç¼€ï¼ˆ`prefix`ï¼‰å·²ç»è¢«â€œç¼“å­˜â€äº†ã€‚æˆ‘ä»¬ç°åœ¨åªæŠŠåç¼€ï¼ˆ`suffix`ï¼‰é€å…¥æ¨¡å‹ã€‚
* æ‰€ä»¥ï¼š`full_att_2d_masks` æ˜¯ä¸€ä¸ª `(B, M, N+M)` å½¢çŠ¶çš„æ©ç ï¼ˆM=åç¼€é•¿åº¦, N=å‰ç¼€é•¿åº¦ï¼‰ã€‚
* å®ƒçš„ä½œç”¨æ˜¯å‘Šè¯‰æ¨¡å‹é‡Œçš„ `ActionExpert` (åŠ¨ä½œå¤§è„‘)ï¼š
  1. ä½ çš„ `suffix` tokenï¼ˆ`M`ä¸ªï¼‰å¯ä»¥å›å¤´çœ‹ `prefix` tokenï¼ˆ`N`ä¸ªï¼‰ã€‚( `prefix_pad_2d_masks` éƒ¨åˆ†)
  2. ä½ çš„ `suffix` token ä¹‹é—´å¿…é¡»å› æœåœ°ï¼ˆcausallyï¼‰ç›¸äº’å…³æ³¨ã€‚( `suffix_att_2d_masks` éƒ¨åˆ†)

Python

```
        prefix_offsets = torch.sum(prefix_pad_masks, dim=-1)[:, None]
        position_ids = prefix_offsets + torch.cumsum(suffix_pad_masks, dim=1) - 1
```

* è§£é‡Šï¼šè®¡ç®— `position_ids`ï¼ˆä½ç½®IDï¼‰ã€‚
* `ActionExpert` éœ€è¦çŸ¥é“ `suffix_embs` æ˜¯åœ¨ `prefix_embs` ä¹‹åçš„ã€‚
* `prefix_offsets` è®¡ç®—å‡ºå‰ç¼€çš„å®é™…é•¿åº¦ï¼ˆæ¯”å¦‚596ï¼‰ã€‚
* `position_ids` å°±ä¼šæ˜¯ `[596, 597, 598, ...]`ã€‚è¿™å‘Šè¯‰ `ActionExpert`ï¼šâ€œä½ ä»¬æ˜¯åºåˆ—ä¸­æ’åœ¨ç¬¬596å·ä½ç½®ä¹‹åçš„tokenâ€ã€‚

Python

```
        # Prepare attention masks
        full_att_2d_masks_4d = self._prepare_attention_masks_4d(full_att_2d_masks)
        ...

        outputs_embeds, _ = self.paligemma_with_expert.forward(
            attention_mask=full_att_2d_masks_4d,
            position_ids=position_ids,
            past_key_values=past_key_values,  # <-- ä¼ å…¥â€œä¾¿ç­¾â€ï¼
            inputs_embeds=[None, suffix_embs],  # <-- åªä¼ å…¥åç¼€ï¼
            use_cache=False,                    # <-- ä¸å†éœ€è¦ç¼“å­˜
            adarms_cond=[None, adarms_cond],
        )
```

* è§£é‡Šï¼šè¿™æ˜¯ `denoise_step` æœ€æ ¸å¿ƒçš„è°ƒç”¨ï¼
* `inputs_embeds=[None, suffix_embs]`ï¼šåªä¼ å…¥åç¼€ã€‚`None` å‘Šè¯‰ `PaliGemma`ï¼ˆè§†è§‰å¤§è„‘ï¼‰ï¼šâ€œä½ è¿™æ¬¡æ²¡äº‹åšï¼Œä¼‘æ¯ã€‚â€
* `past_key_values=past_key_values`ï¼šä¼ å…¥â€œä¾¿ç­¾â€ã€‚
* å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
  1. `PaliGemma` (è§†è§‰å¤§è„‘) å®Œå…¨è·³è¿‡ã€‚
  2. `ActionExpert` (åŠ¨ä½œå¤§è„‘) å¯åŠ¨ã€‚
  3. `ActionExpert` æ¥æ”¶ `suffix_embs` ä½œä¸ºå®ƒçš„æŸ¥è¯¢ï¼ˆQueryï¼‰ã€‚
  4. å®ƒä½¿ç”¨å®ƒè‡ªå·±çš„ `suffix_embs`ï¼ˆç”¨äºåç¼€å†…éƒ¨çš„è‡ªæ³¨æ„åŠ›ï¼‰å’Œ\*\*`past_key_values`ï¼ˆç”¨äºäº¤å‰æ³¨æ„åŠ›ï¼Œå³â€œæŸ¥ä¾¿ç­¾â€ï¼‰ä½œä¸ºé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰\*\*ã€‚
  5. `attention_mask` ç¡®ä¿äº†è¿™ç§æŸ¥è¯¢æ˜¯åˆæ³•çš„ï¼ˆåç¼€å¯ä»¥çœ‹å‰ç¼€ï¼‰ã€‚
  6. `adarms_cond` åœ¨ Pi0.5 æ¨¡å¼ä¸‹ä¼ å…¥æ—¶é—´ $$ $t$ $$ï¼Œæ¥è°ƒèŠ‚ `ActionExpert`ã€‚
* ç»“æœï¼š`ActionExpert` é«˜æ•ˆåœ°è®¡ç®—å‡ºäº†è¾“å‡ºï¼Œå®ƒå®Œå…¨æ²¡æœ‰é‡æ–°è®¡ç®—å›¾åƒå’Œè¯­è¨€ï¼Œåªæ˜¯â€œæŸ¥é˜…â€äº†å®ƒä»¬ã€‚

Python

```
        suffix_out = outputs_embeds[1]
        suffix_out = suffix_out[:, -self.config.action_horizon :]
        suffix_out = suffix_out.to(dtype=torch.float32)
        return self.action_out_proj(suffix_out)
```

* è§£é‡Šï¼šå’Œ `forward` (è®­ç»ƒ) çš„ç»“å°¾ä¸€æ¨¡ä¸€æ ·ã€‚
  1. `outputs_embeds[1]`ï¼š`paligemma_with_expert.forward` çš„è¾“å‡ºæ˜¯ `(prefix_out, suffix_out)`ã€‚`prefix_out` æ­¤æ—¶æ˜¯ `None`ï¼Œæˆ‘ä»¬åªå– `suffix_out`ã€‚
  2. `suffix_out[:, -self.config.action_horizon :]`ï¼šåªä¿ç•™åŠ¨ä½œå¯¹åº”çš„tokenã€‚
  3. `self.action_out_proj(suffix_out)`ï¼šå°†é«˜ç»´è¾“å‡º\*\*æŠ•å½±ï¼ˆå‹ç¼©ï¼‰\*\*å› `32` ç»´çš„åŠ¨ä½œç©ºé—´ã€‚
* è¿”å›ï¼šè¿”å›é¢„æµ‹çš„é€Ÿåº¦ $$ $v_t$ $$ã€‚

è¿™ä¸ª $$ $v_t$ $$ ä¼šè¢«è¿”å›ç»™ `sample_actions` ä¸­çš„ `while` å¾ªç¯ï¼Œç”¨äºæ‰§è¡Œ `x_t = x_t + dt * v_t`ï¼Œä»è€Œå®Œæˆè¿™ä¸€æ­¥çš„â€œé¢å›¢ä¿®æ­£â€ã€‚

***

è‡³æ­¤ï¼Œæˆ‘ä»¬å°±å®Œæ•´åœ°åˆ†æäº† `PI0Pytorch` ç±»çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘ï¼š

1. `__init__`ï¼šæ­å»ºä¸¤ä¸ªå¤§è„‘ï¼ˆPaliGemma, ActionExpertï¼‰ã€‚
2. `embed_prefix` / `embed_suffix`ï¼šå‡†å¤‡â€œèœè°±â€å’Œâ€œåŠ¨ä½œâ€çš„åµŒå…¥ã€‚
3. `make_att_2d_masks`ï¼šåˆ›å»ºå·§å¦™çš„æ³¨æ„åŠ›è§„åˆ™ã€‚
4. `forward` (è®­ç»ƒ)ï¼šçœ‹é¢˜ï¼ˆ$$ $x_t$ $$ï¼‰ã€é¢„æµ‹ï¼ˆ$$ $v_t$ $$ï¼‰ã€å¯¹ç­”æ¡ˆï¼ˆ$$ $u_t$ $$ï¼‰ã€‚
5. `sample_actions` (æ¨ç†)ï¼šå…ˆåšâ€œä¾¿ç­¾â€ï¼ˆKVç¼“å­˜ï¼‰ï¼Œç„¶åå¾ªç¯è°ƒç”¨ `denoise_step`ã€‚
6. `denoise_step` (æ¨ç†æ ¸å¿ƒ)ï¼šâ€œæŸ¥ä¾¿ç­¾â€ã€â€œä¿®é¢å›¢â€ï¼ˆ$$ $x_t$ $$ï¼‰ï¼Œè¿”å› $$ $v_t$ $$ã€‚

