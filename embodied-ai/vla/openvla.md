---
description: 更古早的思考
---

# openVLA

#### ❓ 问题 1：【综合架构】“Pi0.5 vs OpenVLA”

问题： 我们已经深入研究了两个 VLA 模型。请你对比 `VLApi0`（Pi0.5 变体）和 `OpenVLA` 在架构设计上的三个根本区别。

#### 💬 答案 1

“它们代表了 VLA 领域的两种完全不同的技术哲学：

1. 核心算法 (扩散 vs. 自回归)：
   * `VLApi0` (Pi0.5) 是一个扩散模型。它将动作视为连续向量，通过迭代去噪（`denoise_step`）从噪声中“流向”动作。
   * `OpenVLA` 是一个自回归模型。它将动作视为离散词元（token），通过\*\*“下一词元预测”\*\*（`generate`）来“说出”动作 <sup>2</sup>。
2. 模型结构 (双模型 vs. 单模型)：
   * `VLApi0` 是一个\*\*“混合专家”架构，由两个独立\*\*的 Gemma 模型组成：`PaliGemma`（VLM）处理感知，`ActionExpert`（Gemma）处理动作和状态。
   * `OpenVLA` 是一个单一、端到端的模型 <sup>3</sup>。它没有“动作专家”，它就是一个 `Llama 2` VLM <sup>4</sup>，通过“图文缝合”（在 `prismatic.py` 中实现）将视觉信息直接注入到 `Llama` 的上下文窗口中。
3. 状态处理 (使用 `state` vs. 忽略 `state`)：
   * `VLApi0` (Pi0.5) 明确地使用 `time`（时间 $$ $t$ $$）作为 `AdaRMS` 的条件输入来指导去噪。它的 Pi0 变体甚至还使用 `state`（`q_t`，机器人本体感知）。
   * `OpenVLA` 是一个纯粹的“视觉-语言-动作”模型。它既不使用 `time`（自回归模型没有这个概念），也不使用 `state`（`q_t`）<sup>5555</sup>。它必须仅从单张图像（`Input Image`）中推断出机器人的所有状态 <sup>6666</sup>。”

***

#### ❓ 问题 2：【综合训练】“VLA 训练的‘反常识’之处”

问题： `OpenVLA` 论文（Section 3.4）揭示了两个关键的、与标准 LLM/VLM 训练\*\*“常识”相悖的发现。请总结这两个“反常识”的发现，并解释为什么\*\*机器人 VLA 会如此不同？

#### 💬 答案 2

“这两个“反常识”的发现是 `OpenVLA` 的核心贡献，它们揭示了机器人数据与互联网数据的本质区别：

1. 反常识 1：必须微调视觉编码器 (ViT)
   * VLM 常识：`prismatic.py` 的 `freeze_backbones("finetune")` 阶段所代表的策略——冻结 ViT——通常性能更高，因为它能“保护”预训练的视觉知识 <sup>7</sup>。
   * VLA 发现：`OpenVLA` 发现必须使用 `vla-full-train` 策略（解冻 ViT），这“至关重要（crucial）”<sup>8</sup>。
   * 原因：论文推测，互联网 ViT（`SigLIP`）擅长语义（“杯子”），但机器人需要\*\*“精细的空间细节”（“杯柄在 `(x,y,z)`”）<sup>9</sup>。`DinoV2` 提供了空间特征 <sup>10</sup>，但仍必须通过微调来“激活”这些特征以用于精确操控\*\*。
2. 反常识 2：需要极多的训练周期 (Epochs)
   * LLM/VLM 常识：在海量数据上只训练 1-2 个周期 <sup>11</sup>。
   * VLA 发现：`OpenVLA` 训练了 27 个周期，并且性能“持续增长”<sup>12</sup>。
   * 原因：这揭示了“机器人数据”是\*\*“低密度（low-density）”的。在 97 万个轨迹中，大量数据是“无聊的”（例如等待、微小移动）。模型需要 27 次“反复刷”数据，才能从中“筛”出那些真正关键\*\*的、“高密度”的动作时刻（如“抓取”或“放置”的瞬间）。”

***

#### ❓ 问题 3：【综合应用】“让 VLA 飞入寻常百姓家”

问题： `OpenVLA` 的最终目标是“可访问性（Accessibility）”。论文（Section 5.3 & 5.4）和代码（`prismatic.py` 中的 FSDP，以及 `peft` 库）共同实现了这一目标。请总结 `LoRA` 和 `int4 量化`是如何分别解决“训练”和“推理”的计算难题的？

#### 💬 答案 3

“`LoRA` 和 `int4` 量化是 `OpenVLA` 得以“开源”并被社区真正使用的两个“效率支柱”：

1. LoRA（解决“训练”难题）<sup>13</sup>：
   * 问题：全参数微调（Full FT）一个 7B 模型需要海量 VRAM (163.3 GB) 和计算资源 <sup>1414</sup>。
   * LoRA (低秩自适应)：这是一种参数高效微调（PEFT）技术，它冻结所有 70 亿参数，只\*\*“加装”并训练极少数\*\*（约 1.4%）的“适配器”权重 <sup>15</sup>。
   * 结果：`OpenVLA` 发现 LoRA 的性能（68.2%）与全量微调（69.7%）几乎相同，但训练 VRAM 降低了 63%（降至 59.7 GB），计算量减少了 8 倍 <sup>16161616</sup>。这使得在单张高端 GPU 上微调成为可能。
2. `int4` 量化（解决“推理”难题）<sup>17</sup>：
   * 问题：7B 模型即使在 `bfloat16`（半精度）下推理，也需要 16.8 GB VRAM <sup>1818</sup>，这超出了许多消费级 GPU。
   * 量化（Quantization）：将模型的权重从 16-bit 压缩到 4-bit。
   * 结果：`OpenVLA` 发现 `int4` 量化几乎没有性能损失（71.9% vs 71.3%），但 VRAM 占用减半，降至 7.0 GB 。
   * 结论：这使得 `OpenVLA` 可以在消费级 GPU（如 RTX 4090）上以实时（约 6Hz）的速度运行 。
